alertmanager:
  alertmanagerSpec:
    replicas: 1
    externalUrl: $ALERTMANAGER_URL
    logLevel: debug
  config:
    global:
      resolve_timeout: 5m
    route:
      receiver: "am-webhook"
      group_by: ['alertname']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
      - match:
          alertname: Watchdog
        receiver: 'null'
      - match:
          alertname: TestAlert
        receiver: 'am-webhook'
    receivers:
    - name: 'null'
    - name: "am-webhook"
      webhook_configs:
      - url: 'http://alert-webhook:80/webhook'
        send_resolved: true
    # - name: 'email'
    #   email_configs:
    #   - to: $GMAIL_ACCOUNT
    #     from: $GMAIL_ACCOUNT
    #     smarthost: "smtp.gmail.com:587"
    #     auth_username: "$GMAIL_ACCOUNT"
    #     auth_identity: "$GMAIL_ACCOUNT"
    #     auth_password: "$GMAIL_AUTH_TOKEN"
  # templateFiles:
  #   template_1.tmpl: |-
  #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}

  #       {{ define "slack.myorg.text" }}
  #       {{- $root := . -}}
  #       {{ range .Alerts }}
  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
  #         *Cluster:*  {{ template "cluster" $root }}
  #         *Description:* {{ .Annotations.description }}
  #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
  #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
  #         *Details:*
  #           {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
  #           {{ end }}

prometheus:
  prometheusSpec:
    configMaps:
      - bosh-target-groups
    replicas: 1
    externalUrl: $PROMETHEUS_URL
    externalLabels:
      cluster: "$CLUSTER_NAME"
      foundation: "$FOUNDATION"
    secrets:
      - etcd-client
    ruleSelector:
      matchLabels:
        app: prometheus-operator
        release: prometheus-operator
    enableAdminAPI: true
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: thin-disk
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi
        selector:
          matchExpressions:
            - key: app
              operator: In
              values:
                - prometheus

grafana:
  replicas: 1
  # ingress:
  #   enabled: true
  notifiers: 
    notifiers.yaml:
      notifiers:
      - name: email-notifier
        type: email
        uid: email1
        org_id: 1
        is_default: true
        settings:
          addresses: "$GMAIL_ACCOUNT"

  smtp:
    existingSecret: "smtp-creds"
    userKey: "user"
    passwordKey: "password"

  grafana.ini:
    smtp:
      enabled: true
      userKey: "user"
      passwordKey: "password"
      host: "smtp.gmail.com:587"

additionalPrometheusRulesMap:
  testing:
    groups:
    - name: test-alerting.rules
      rules:
      - alert: TestAlert
        annotations:
          message: The disk {{`{{ $labels.instance }}`}} running on {{`{{ $labels.cluster }}`}} in {{`{{ $labels.foundation }}`}} is running full.
        expr: vector(1)
        for: 1m
        labels:
          severity: critical

kubeApiServer:
  serviceMonitor:
    interval: 30s
    metricRelabelings:
    - action: drop
      regex: etcd_(debugging|disk|request|server).*
      sourceLabels:
      - __name__
    - action: drop
      regex: apiserver_admission_controller_admission_latencies_seconds_.*
      sourceLabels:
      - __name__
    - action: drop
      regex: apiserver_admission_step_admission_latencies_seconds_.*
      sourceLabels:
      - __name__

kubeControllerManager:
  endpoints: ${ENDPOINTS}
  serviceMonitor:
    interval: 30s
    insecureSkipVerify: true
    metricRelabelings:
    - action: drop
      regex: etcd_(debugging|disk|request|server).*
      sourceLabels:
      - __name__

kubeScheduler:
  endpoints: ${ENDPOINTS}
  serviceMonitor:
    interval: 30s
    insecureSkipVerify: true
